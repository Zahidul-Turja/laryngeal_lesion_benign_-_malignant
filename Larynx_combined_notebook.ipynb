{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "NAkXv_pDwIVg",
        "uFDCZAaMvoRJ",
        "tjSh7jdk4ILl",
        "Fpck8pz-v7ij",
        "fSoPkMniyXOj",
        "KBlcKqEN4nce",
        "IYS4Qwzv20WI",
        "rNRWI2IqT1kp",
        "3rMKM-zoUn8C",
        "JLJ2VZXb226D",
        "DHH9rcS1yaUo"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "NAkXv_pDwIVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "import keras"
      ],
      "metadata": {
        "id": "ERddHgokwKY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utility functions"
      ],
      "metadata": {
        "id": "uFDCZAaMvoRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Callbacks"
      ],
      "metadata": {
        "id": "bpg7wJeLv2Dg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\",\n",
        "                                                  patience=3)"
      ],
      "metadata": {
        "id": "dl98Oh_s1Pia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\",\n",
        "                                                 factor=0.2,\n",
        "                                                 patience=2,\n",
        "                                                 verbose=1,\n",
        "                                                 min_lr=1e-7)"
      ],
      "metadata": {
        "id": "7CQybqwM1a-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Genaration"
      ],
      "metadata": {
        "id": "JCTDIKlY1oiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_data(train_dir = \"/content/drive/MyDrive/larynx_dataset/train\",\n",
        "                  test_dir = \"/content/drive/MyDrive/larynx_dataset/test\",\n",
        "                  valid_dir = \"/content/drive/MyDrive/larynx_dataset/validation\"):\n",
        "  datagen = ImageDataGenerator(rescale=1/255.)\n",
        "\n",
        "  train_data = datagen.flow_from_directory(directory=train_dir,\n",
        "                                                batch_size=32,\n",
        "                                                target_size=(224, 224),\n",
        "                                                class_mode=\"binary\",\n",
        "                                                seed=42)\n",
        "  test_data = datagen.flow_from_directory(directory=test_dir,\n",
        "                                                batch_size=32,\n",
        "                                                target_size=(224, 224),\n",
        "                                                class_mode=\"binary\",\n",
        "                                                seed=42)\n",
        "  validation_data = datagen.flow_from_directory(directory=valid_dir,\n",
        "                                                batch_size=32,\n",
        "                                                target_size=(224, 224),\n",
        "                                                class_mode=\"binary\",\n",
        "                                                seed=42)\n",
        "\n",
        "  return train_data, test_data, validation_data"
      ],
      "metadata": {
        "id": "pDU5fwor1rev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Models Build"
      ],
      "metadata": {
        "id": "5kW9EbvtwAHD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-trained models"
      ],
      "metadata": {
        "id": "tjSh7jdk4ILl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(model_url, num_classes=1, IMAGE_SHAPE=(224, 224), BATCH_SIZE=32):\n",
        "  feature_extractor_layer = hub.KerasLayer(model_url,\n",
        "                                           trainable=False,\n",
        "                                           name=\"feature_extraction_layer\",\n",
        "                                           input_shape=IMAGE_SHAPE+(3, ))\n",
        "  model = tf.keras.Sequential([\n",
        "      feature_extractor_layer,\n",
        "      layers.Dense(num_classes, activation=\"sigmoid\", name=\"output_layer\")\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "nmcB_N3T1ld3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble"
      ],
      "metadata": {
        "id": "LTzh6Cx24OA9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble(models, model_input):\n",
        "  Models_output = [model(model_input) for model in models]\n",
        "  AVG = layers.average(Models_output)\n",
        "\n",
        "  model_ensemble = keras.models.Model(inputs=model_input, outputs=AVG, name=\"ensemble\")\n",
        "  model_ensemble.summary()\n",
        "  model_ensemble.compile(loss=\"binary_crossentropy\",\n",
        "                          optimizer=tf.keras.optimizers.Adam(),\n",
        "                          metrics=[\"accuracy\"])\n",
        "\n",
        "  return model_ensemble"
      ],
      "metadata": {
        "id": "2Y78BX2X4GNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot and Curves"
      ],
      "metadata": {
        "id": "Fpck8pz-v7ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_all_images(images_list, titles):\n",
        "  rows = 4\n",
        "  columns = 4\n",
        "\n",
        "  fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "  for i in range(16):\n",
        "    img = mpimg.imread(images_list[i])\n",
        "\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(titles[i])"
      ],
      "metadata": {
        "id": "eEv9Aiffx00R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_accuracy_curves(historys, titles):\n",
        "  fig = plt.figure(figsize=(len(historys) * 5, 3))\n",
        "  rows = 1\n",
        "  columns = len(historys)\n",
        "\n",
        "  for i in range(len(historys)):\n",
        "    loss = historys[i][\"loss\"]\n",
        "    val_loss = historys[i][\"val_loss\"]\n",
        "\n",
        "    accuracy = historys[i][\"accuracy\"]\n",
        "    val_accuracy = historys[i][\"val_accuracy\"]\n",
        "\n",
        "    epochs = range(len(historys[i][\"loss\"]))\n",
        "\n",
        "    fig.add_subplot(rows, columns, i+1)\n",
        "\n",
        "    plt.plot(epochs, accuracy, label=\"accuracy\", color=\"lightseagreen\")\n",
        "    plt.plot(epochs, val_accuracy, label=\"val_accuracy\", color=\"salmon\")\n",
        "    plt.title(titles[i])\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "L27_fzk6x8CJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curves(historys, titles, colors):\n",
        "  fig = plt.figure(figsize=(10, 3))\n",
        "  rows = 1\n",
        "  columns = 2\n",
        "\n",
        "  fig.add_subplot(rows, columns, 1)\n",
        "  for i in range(len(historys)):\n",
        "    accuracy = historys[i][\"accuracy\"]\n",
        "    val_accuracy = historys[i][\"val_accuracy\"]\n",
        "\n",
        "    epochs = range(len(historys[i][\"loss\"]))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.plot(epochs, accuracy, label=titles[i], color=colors[i])\n",
        "    plt.title(\"Training Accuracy\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "  fig.add_subplot(rows, columns, 2)\n",
        "  for i in range(len(historys)):\n",
        "    loss = historys[i][\"loss\"]\n",
        "    val_loss = historys[i][\"val_loss\"]\n",
        "\n",
        "    epochs = range(len(historys[i][\"loss\"]))\n",
        "\n",
        "    # Plot loss\n",
        "    plt.plot(epochs, loss, label=titles[i], color=colors[i])\n",
        "    plt.title(\"Training Loss\")\n",
        "    plt.xlabel(\"epochs\")\n",
        "    plt.legend()"
      ],
      "metadata": {
        "id": "5hBkIc6XyIBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create and Save Models"
      ],
      "metadata": {
        "id": "fSoPkMniyXOj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pre-trained Models"
      ],
      "metadata": {
        "id": "mHL60hpi2we4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Original Dataset"
      ],
      "metadata": {
        "id": "hejV4ZJ64goF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Generate Data"
      ],
      "metadata": {
        "id": "AyF6ExqL4sq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, validation_data = generate_data(\n",
        "                  train_dir = \"/content/drive/MyDrive/larynx_dataset_original/train\",\n",
        "                  test_dir = \"/content/drive/MyDrive/larynx_dataset_original/test\",\n",
        "                  valid_dir = \"/content/drive/MyDrive/larynx_dataset_original/validation\"\n",
        ")"
      ],
      "metadata": {
        "id": "r4iJIREe5nHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### MobileNet V3"
      ],
      "metadata": {
        "id": "YIk7lEtX5zIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_v3 = \"https://www.kaggle.com/models/google/mobilenet-v3/frameworks/TensorFlow2/variations/large-075-224-feature-vector/versions/1\"\n",
        "\n",
        "# Create Resnet model\n",
        "mobilenet_v3_model = create_model(mobilenet_v3)\n",
        "\n",
        "# Compile model\n",
        "mobilenet_v3_model.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])\n",
        "# Create an empty dictionary to store the training history\n",
        "mobilenet_aug_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        mobilenet_aug_history['loss'].append(logs['loss'])\n",
        "        mobilenet_aug_history['accuracy'].append(logs['accuracy'])\n",
        "        mobilenet_aug_history['val_loss'].append(logs['val_loss'])\n",
        "        mobilenet_aug_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Fit the model\n",
        "mobilenet_v3_history = mobilenet_v3_model.fit(train_data,\n",
        "                                  epochs=50,\n",
        "                                  steps_per_epoch=len(train_data),\n",
        "                                  validation_data=validation_data,\n",
        "                                  validation_steps=len(validation_data),\n",
        "                                  callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(mobilenet_aug_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/mobilenet_main_history.csv', index=False)\n",
        "\n",
        "mobilenet_v3_model.save(\"/content/drive/MyDrive/models/mobilenet_main\")\n",
        "\n",
        "mobilenet_v3.evaluate(test_data)"
      ],
      "metadata": {
        "id": "VqFJzlOW53v2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### EfficientNetB0 V2"
      ],
      "metadata": {
        "id": "n_8jag6ERo75"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "\n",
        "# Create EfficientNetB0 feature extractor model\n",
        "efficientnet_model_main = create_model(model_url=efficientnet_url)\n",
        "\n",
        "# Compile model\n",
        "efficientnet_model_main.compile(loss=\"binary_crossentropy\",\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "# Create an empty dictionary to store the training history\n",
        "efficient_main_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        efficient_main_history['loss'].append(logs['loss'])\n",
        "        efficient_main_history['accuracy'].append(logs['accuracy'])\n",
        "        efficient_main_history['val_loss'].append(logs['val_loss'])\n",
        "        efficient_main_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Fit model\n",
        "efficientnet_history = efficientnet_model_main.fit(train_data,\n",
        "                                              epochs=50,\n",
        "                                              steps_per_epoch=len(train_data),\n",
        "                                              validation_data=validation_data,\n",
        "                                              validation_steps=len(validation_data),\n",
        "                                              callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(efficient_main_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/efficient_original_history.csv', index=False)\n",
        "\n",
        "efficientnet_model_main.save(\"/content/drive/MyDrive/models/efficientnet_original\")\n",
        "\n",
        "efficientnet_model_main.evaluate(test_data)"
      ],
      "metadata": {
        "id": "FAZ_NRaRRtnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ResNet50 V2"
      ],
      "metadata": {
        "id": "7aDUYCkKTJH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n",
        "\n",
        "# Create resnet feature extractor model\n",
        "resnet_model_original = create_model(model_url=resnet_url)\n",
        "\n",
        "# Compile model\n",
        "resnet_model_original.compile(loss=\"binary_crossentropy\",\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "# Create an empty dictionary to store the training history\n",
        "resnet_original_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        resnet_original_history['loss'].append(logs['loss'])\n",
        "        resnet_original_history['accuracy'].append(logs['accuracy'])\n",
        "        resnet_original_history['val_loss'].append(logs['val_loss'])\n",
        "        resnet_original_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Fit model\n",
        "resnet_history = resnet_model_original.fit(train_data,\n",
        "                                              epochs=50,\n",
        "                                              steps_per_epoch=len(train_data),\n",
        "                                              validation_data=validation_data,\n",
        "                                              validation_steps=len(validation_data),\n",
        "                                              callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(resnet_original_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/resnet_original_history.csv', index=False)\n",
        "\n",
        "resnet_model_original.save(\"/content/drive/MyDrive/models/resnet_original\")\n",
        "\n",
        "resnet_model_original.evaluate(test_data)"
      ],
      "metadata": {
        "id": "Frq2pKJMTMVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Augmented Dataset"
      ],
      "metadata": {
        "id": "KBlcKqEN4nce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Generate Data"
      ],
      "metadata": {
        "id": "SMTkG3T94vHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data, validation_data = generate_data(\n",
        "                  train_dir = \"/content/drive/MyDrive/larynx_dataset_augmented/train\",\n",
        "                  test_dir = \"/content/drive/MyDrive/larynx_dataset_augmented/test\",\n",
        "                  valid_dir = \"/content/drive/MyDrive/larynx_dataset_augmented/validation\"\n",
        ")"
      ],
      "metadata": {
        "id": "FqC54p1vROTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### MobileNet v3"
      ],
      "metadata": {
        "id": "sF3gb-kXRaDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_v3 = \"https://www.kaggle.com/models/google/mobilenet-v3/frameworks/TensorFlow2/variations/large-075-224-feature-vector/versions/1\"\n",
        "\n",
        "# Create Resnet model\n",
        "mobilenet_v3_model = create_model(mobilenet_v3)\n",
        "\n",
        "# Compile model\n",
        "mobilenet_v3_model.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=tf.keras.optimizers.Adam(),\n",
        "                     metrics=[\"accuracy\"])\n",
        "# Create an empty dictionary to store the training history\n",
        "mobilenet_aug_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        mobilenet_aug_history['loss'].append(logs['loss'])\n",
        "        mobilenet_aug_history['accuracy'].append(logs['accuracy'])\n",
        "        mobilenet_aug_history['val_loss'].append(logs['val_loss'])\n",
        "        mobilenet_aug_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Fit the model\n",
        "mobilenet_v3_history = mobilenet_v3_model.fit(train_data,\n",
        "                                  epochs=50,\n",
        "                                  steps_per_epoch=len(train_data),\n",
        "                                  validation_data=validation_data,\n",
        "                                  validation_steps=len(validation_data),\n",
        "                                  callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(mobilenet_aug_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/mobilenet_augmented_history.csv', index=False)\n",
        "\n",
        "mobilenet_v3_model.save(\"/content/drive/MyDrive/models/mobilenet_augmented\")\n",
        "\n",
        "mobilenet_v3.evaluate(test_data)"
      ],
      "metadata": {
        "id": "SqtbZEO5Rdl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### EfficientNetB0 V2"
      ],
      "metadata": {
        "id": "ImcNvLgDSI5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "efficientnet_url = \"https://tfhub.dev/tensorflow/efficientnet/b0/feature-vector/1\"\n",
        "\n",
        "# Create EfficientNetB0 feature extractor model\n",
        "efficientnet_model_main = create_model(model_url=efficientnet_url)\n",
        "\n",
        "# Compile model\n",
        "efficientnet_model_main.compile(loss=\"binary_crossentropy\",\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "# Create an empty dictionary to store the training history\n",
        "efficient_main_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        efficient_main_history['loss'].append(logs['loss'])\n",
        "        efficient_main_history['accuracy'].append(logs['accuracy'])\n",
        "        efficient_main_history['val_loss'].append(logs['val_loss'])\n",
        "        efficient_main_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Fit model\n",
        "efficientnet_history = efficientnet_model_main.fit(train_data,\n",
        "                                              epochs=50,\n",
        "                                              steps_per_epoch=len(train_data),\n",
        "                                              validation_data=validation_data,\n",
        "                                              validation_steps=len(validation_data),\n",
        "                                              callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(efficient_main_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/efficient_augmented_history.csv', index=False)\n",
        "\n",
        "efficientnet_model_main.save(\"/content/drive/MyDrive/models/efficientnet_augmented\")\n",
        "\n",
        "efficientnet_model_main.evaluate(test_data)"
      ],
      "metadata": {
        "id": "TKyrJYZmSMqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### ResNet50 V2"
      ],
      "metadata": {
        "id": "8oDv83y1SRWK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_url = \"https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/5\"\n",
        "\n",
        "# Create resnet feature extractor model\n",
        "resnet_model_augmented = create_model(model_url=resnet_url)\n",
        "\n",
        "# Compile model\n",
        "resnet_model_augmented.compile(loss=\"binary_crossentropy\",\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=[\"accuracy\"])\n",
        "\n",
        "# Create an empty dictionary to store the training history\n",
        "resnet_augmented_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        resnet_augmented_history['loss'].append(logs['loss'])\n",
        "        resnet_augmented_history['accuracy'].append(logs['accuracy'])\n",
        "        resnet_augmented_history['val_loss'].append(logs['val_loss'])\n",
        "        resnet_augmented_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Fit model\n",
        "resnet_history = resnet_model_augmented.fit(train_data,\n",
        "                                              epochs=50,\n",
        "                                              steps_per_epoch=len(train_data),\n",
        "                                              validation_data=validation_data,\n",
        "                                              validation_steps=len(validation_data),\n",
        "                                              callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(resnet_augmented_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/resnet_augmented_history.csv', index=False)\n",
        "\n",
        "resnet_model_augmented.save(\"/content/drive/MyDrive/models/resnet_augmented\")\n",
        "\n",
        "resnet_model_augmented.evaluate(test_data)"
      ],
      "metadata": {
        "id": "UrJGsRAUST2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ensemble Models"
      ],
      "metadata": {
        "id": "IYS4Qwzv20WI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Original Dataset"
      ],
      "metadata": {
        "id": "rNRWI2IqT1kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_model = keras.models.load_model(\"/content/drive/MyDrive/models/mobilenet_orginal\")\n",
        "efficientnet_model = keras.models.load_model(\"/content/drive/MyDrive/models/efficientnet_original\")\n",
        "resnet_loaded_model = keras.models.load_model(\"/content/drive/MyDrive/models/resnet_original\")\n",
        "\n",
        "models = []\n",
        "\n",
        "models.append(mobilenet_model)\n",
        "models.append(efficientnet_model)\n",
        "models.append(resnet_loaded_model)\n",
        "\n",
        "model_input = layers.Input(shape=models[0].input_shape[1:])\n",
        "\n",
        "ensemble_model = ensemble(models, model_input)\n",
        "\n",
        "scores = ensemble_model.evaluate(test_data,\n",
        "                                  steps=len(test_data))\n",
        "print(\"Mobile_v3 + Efficient_B0 + ResNet_v2 Accuracy = \", scores[1])"
      ],
      "metadata": {
        "id": "NQZNrAcST5Ce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Augmented Dataset"
      ],
      "metadata": {
        "id": "3rMKM-zoUn8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mobilenet_model = keras.models.load_model(\"/content/drive/MyDrive/models/mobilenet_augmented\")\n",
        "efficientnet_model = keras.models.load_model(\"/content/drive/MyDrive/models/efficientnet_augmented\")\n",
        "resnet_loaded_model = keras.models.load_model(\"/content/drive/MyDrive/models/resnet_augmented\")\n",
        "\n",
        "models = []\n",
        "\n",
        "models.append(mobilenet_model)\n",
        "models.append(efficientnet_model)\n",
        "models.append(resnet_loaded_model)\n",
        "\n",
        "model_input = layers.Input(shape=models[0].input_shape[1:])\n",
        "\n",
        "ensemble_model = ensemble(models, model_input)\n",
        "\n",
        "scores = ensemble_model.evaluate(test_data,\n",
        "                                  steps=len(test_data))\n",
        "print(\"Mobile_v3 + Efficient_B0 + ResNet_v2 Accuracy = \", scores[1])"
      ],
      "metadata": {
        "id": "mPeq4DgsUrdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stack Models"
      ],
      "metadata": {
        "id": "JLJ2VZXb226D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Original Dataset"
      ],
      "metadata": {
        "id": "Wx-4xP6UU4bw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "mobilenet_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/mobilenet_original\")\n",
        "efficientnet_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/efficientnet_original\")\n",
        "resnet_loaded_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/resnet_original\")\n",
        "\n",
        "models = [mobilenet_model, efficientnet_model, resnet_loaded_model]\n",
        "\n",
        "# Freeze the layers of base models\n",
        "for model in models:\n",
        "    model.trainable = False\n",
        "\n",
        "models[0]._name = 'mobilenet_model_original'\n",
        "models[1]._name = 'efficientnet_model_original'\n",
        "models[2]._name = 'resnet_model_original'\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    for layer in model.layers:\n",
        "        layer._name = f\"{model.name}_layer_{layer.name}\"\n",
        "\n",
        "# Create a stacking model\n",
        "model_input = layers.Input(shape=(224, 224, 3))  # Input shape should match the input shape of your images\n",
        "mobilenet_output = mobilenet_model(model_input)\n",
        "efficientnet_output = efficientnet_model(model_input)\n",
        "resnet_output = resnet_loaded_model(model_input)\n",
        "\n",
        "# Concatenate the outputs of the base models\n",
        "concatenated = layers.Concatenate(axis=-1)([mobilenet_output, efficientnet_output, resnet_output])\n",
        "\n",
        "# Add a dense layer for the meta-model\n",
        "stacked_model_output = layers.Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Create the stacked model\n",
        "stacked_model = tf.keras.models.Model(inputs=model_input, outputs=stacked_model_output, name=\"stacked_model\")\n",
        "\n",
        "stacked_model.compile(loss=\"binary_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "stacked_model.summary()\n",
        "\n",
        "# Create an empty dictionary to store the training history\n",
        "stacked_original_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        stacked_original_history['loss'].append(logs['loss'])\n",
        "        stacked_original_history['accuracy'].append(logs['accuracy'])\n",
        "        stacked_original_history['val_loss'].append(logs['val_loss'])\n",
        "        stacked_original_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Train the stacked model\n",
        "stacked_model.fit(train_data,\n",
        "                  epochs=50,  # Adjust the number of epochs based on your requirements\n",
        "                  validation_data=validation_data,\n",
        "                  steps_per_epoch=len(train_data),\n",
        "                  validation_steps=len(validation_data),\n",
        "                  callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(stacked_original_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/stacked_original_history.csv', index=False)\n",
        "\n",
        "stacked_model.save(\"/content/drive/MyDrive/models/stacked_original\")\n",
        "\n",
        "scores = stacked_model.evaluate(test_data, steps=len(test_data))\n",
        "print(\"Stacked Model Accuracy = \", scores[1])"
      ],
      "metadata": {
        "id": "QUZx5JgpVAwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Augmented Dataset"
      ],
      "metadata": {
        "id": "tzMaNJbGU9a4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model\n",
        "mobilenet_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/mobilenet_augmented\")\n",
        "efficientnet_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/efficientnet_augmented\")\n",
        "resnet_loaded_model = tf.keras.models.load_model(\"/content/drive/MyDrive/models/resnet_augmented\")\n",
        "\n",
        "models = [mobilenet_model, efficientnet_model, resnet_loaded_model]\n",
        "\n",
        "# Freeze the layers of base models\n",
        "for model in models:\n",
        "    model.trainable = False\n",
        "\n",
        "models[0]._name = 'mobilenet_model_augmented'\n",
        "models[1]._name = 'efficientnet_model_augmented'\n",
        "models[2]._name = 'resnet_model_augmented'\n",
        "\n",
        "for i, model in enumerate(models):\n",
        "    for layer in model.layers:\n",
        "        layer._name = f\"{model.name}_layer_{layer.name}\"\n",
        "\n",
        "# Create a stacking model\n",
        "model_input = layers.Input(shape=(224, 224, 3))  # Input shape should match the input shape of your images\n",
        "mobilenet_output = mobilenet_model(model_input)\n",
        "efficientnet_output = efficientnet_model(model_input)\n",
        "resnet_output = resnet_loaded_model(model_input)\n",
        "\n",
        "# Concatenate the outputs of the base models\n",
        "concatenated = layers.Concatenate(axis=-1)([mobilenet_output, efficientnet_output, resnet_output])\n",
        "\n",
        "# Add a dense layer for the meta-model\n",
        "stacked_model_output = layers.Dense(1, activation='sigmoid')(concatenated)\n",
        "\n",
        "# Create the stacked model\n",
        "stacked_model = tf.keras.models.Model(inputs=model_input, outputs=stacked_model_output, name=\"stacked_model\")\n",
        "\n",
        "stacked_model.compile(loss=\"binary_crossentropy\",\n",
        "                      optimizer=tf.keras.optimizers.Adam(),\n",
        "                      metrics=[\"accuracy\"])\n",
        "\n",
        "stacked_model.summary()\n",
        "\n",
        "# Create an empty dictionary to store the training history\n",
        "stacked_augmented_history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
        "\n",
        "# Define a callback to update the history dictionary during training\n",
        "class HistoryCallback(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        stacked_augmented_history['loss'].append(logs['loss'])\n",
        "        stacked_augmented_history['accuracy'].append(logs['accuracy'])\n",
        "        stacked_augmented_history['val_loss'].append(logs['val_loss'])\n",
        "        stacked_augmented_history['val_accuracy'].append(logs['val_accuracy'])\n",
        "\n",
        "history_callback = HistoryCallback()\n",
        "\n",
        "# Train the stacked model\n",
        "stacked_model.fit(train_data,\n",
        "                  epochs=50,  # Adjust the number of epochs based on your requirements\n",
        "                  validation_data=validation_data,\n",
        "                  steps_per_epoch=len(train_data),\n",
        "                  validation_steps=len(validation_data),\n",
        "                  callbacks=[history_callback, early_stopping, reduce_lr])\n",
        "\n",
        "history_df = pd.DataFrame(stacked_augmented_history)\n",
        "history_df.to_csv('/content/drive/MyDrive/model_history/stacked_augmented_history.csv', index=False)\n",
        "\n",
        "stacked_model.save(\"/content/drive/MyDrive/models/stacked_augmented\")\n",
        "\n",
        "scores = stacked_model.evaluate(test_data, steps=len(test_data))\n",
        "print(\"Stacked Model Accuracy = \", scores[1])"
      ],
      "metadata": {
        "id": "e5cggMW4VBY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Plots"
      ],
      "metadata": {
        "id": "DHH9rcS1yaUo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot images of all subclasses alphabetically"
      ],
      "metadata": {
        "id": "1CaQ3lO4z10y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = [\n",
        "    '/content/drive/MyDrive/438_plot_images/Amyloidosis/P056 (2).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Carcinoma_in_situ/P013 (38).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Cyst/P037 (42).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Granuloma/P167 (5).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Hemangioma/P123 (42).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/High_grade_dyspasia/P142 (4).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Hyperkeratosis/P032 (20).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Hyperplasia/P053 (6).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Inflammation/P090 (5).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Low_grade_dyspasia/P129 (16).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Namboo_node/P208 (15).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Nodule/P049 (22).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Papillomatosis/P133 (26).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Polyp/P038 (29).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/Reinkes_edema/P029 (15).jpg',\n",
        "    '/content/drive/MyDrive/438_plot_images/SCC/P001 (5).jpg',\n",
        "]\n",
        "\n",
        "titles = [\n",
        "    \"Amyloidosis\",\n",
        "    \"Carcinoma in situ\",\n",
        "    \"Cyst\",\n",
        "    \"Granuloma\",\n",
        "    \"Hemangioma\",\n",
        "    \"High grade dyspasia\",\n",
        "    \"Hyperkeratosis\",\n",
        "    \"Hyperplasia\",\n",
        "    \"Inflammation\",\n",
        "    \"Low grade dyspasia\",\n",
        "    \"Namboo node\",\n",
        "    \"Nodule\",\n",
        "    \"Papillomatosis\",\n",
        "    \"Polyp\",\n",
        "    \"Renkes edema\",\n",
        "    \"SCC\",\n",
        "]\n",
        "\n",
        "plot_all_images(img_list, titles)"
      ],
      "metadata": {
        "id": "zu99gw4-yj-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare Biased Classes before and after augmentation"
      ],
      "metadata": {
        "id": "_nFlDi8nz570"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = ['Raw Dataset','Augmented Dataset']\n",
        "Benign = [7657, 7657]\n",
        "Malignant = [3487, 692+4872+1384]\n",
        "\n",
        "X_axis = np.arange(len(X))\n",
        "\n",
        "plt.bar(X_axis - 0.2, Benign, 0.4, label = 'Benign', color=\"salmon\")\n",
        "plt.bar(X_axis + 0.2, Malignant, 0.4, label = 'Malignant', color=\"lightseagreen\")\n",
        "\n",
        "plt.xticks(X_axis, X)\n",
        "plt.ylim(0, 10000)\n",
        "plt.xlabel(\"Type\")\n",
        "plt.ylabel(\"Number of Images\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tmIiY6AN0EfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot Accuracy Curves of Stacked Models"
      ],
      "metadata": {
        "id": "JXHdePC70OUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_aug = pd.read_csv(\"/content/drive/MyDrive/model_history/stacked_aug_history.csv\")\n",
        "stacked_main = pd.read_csv(\"/content/drive/MyDrive/model_history/stacked_main_history.csv\")\n",
        "\n",
        "historys = []\n",
        "historys.append(stacked_aug)\n",
        "historys.append(stacked_main)\n",
        "\n",
        "plot_accuracy_curves(history, [\"Stacked Aug\", \"Stacked Raw\"])"
      ],
      "metadata": {
        "id": "J303kFwq0U5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot Accuracy Curves of Base Models on Augmented Dataset"
      ],
      "metadata": {
        "id": "wBem7-yI0bAl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historys = []\n",
        "\n",
        "mobile_net = pd.read_csv(\"/content/drive/MyDrive/model_history/mobilenet_aug_history.csv\")\n",
        "efficient_net = pd.read_csv(\"/content/drive/MyDrive/model_history/efficient_aug_history.csv\")\n",
        "res_net = pd.read_csv(\"/content/drive/MyDrive/model_history/resnet_aug.csv\")\n",
        "\n",
        "historys.append(mobile_net)\n",
        "historys.append(efficient_net)\n",
        "historys.append(res_net)\n",
        "\n",
        "plot_accuracy_curves(historys, [\"MobileNet V3\", \"EfficientNetB0 V2\", \"ResNet50 V2\"])"
      ],
      "metadata": {
        "id": "_1ENCvy90htP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot Accuracy Curves of Base Models on Original Dataset"
      ],
      "metadata": {
        "id": "Q4YbYgS80rIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historys = []\n",
        "\n",
        "mobile_net = pd.read_csv(\"/content/drive/MyDrive/model_history/mobilenet_main_history.csv\")\n",
        "efficient_net = pd.read_csv(\"/content/drive/MyDrive/model_history/efficient_main_history.csv\")\n",
        "res_net = pd.read_csv(\"/content/drive/MyDrive/model_history/resnet_main.csv\")\n",
        "\n",
        "historys.append(mobile_net)\n",
        "historys.append(efficient_net)\n",
        "historys.append(res_net)\n",
        "\n",
        "plot_accuracy_curves(historys, [\"MobileNet V3\", \"EfficientNetB0 V2\", \"ResNet50 V2\"])"
      ],
      "metadata": {
        "id": "pYSMXWW90srs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot Accuracy and Loss Curves of all Models"
      ],
      "metadata": {
        "id": "DhxgPs6b036s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "historys = []\n",
        "\n",
        "mobile_net = pd.read_csv(\"/content/drive/MyDrive/model_history/mobilenet_aug_history.csv\")\n",
        "efficient_net = pd.read_csv(\"/content/drive/MyDrive/model_history/efficient_aug_history.csv\")\n",
        "res_net = pd.read_csv(\"/content/drive/MyDrive/model_history/resnet_aug.csv\")\n",
        "stacked_aug = pd.read_csv(\"/content/drive/MyDrive/model_history/stacked_aug_history.csv\")\n",
        "stacked_main = pd.read_csv(\"/content/drive/MyDrive/model_history/stacked_main_history.csv\")\n",
        "\n",
        "titles = [\"MobileNet\", \"EfficientNet\", \"ResNet\", \"Stacked\", \"Stacked_aug\"]\n",
        "colors = [\"orchid\", \"orange\", \"mediumslateblue\", \"mediumseagreen\", \"salmon\"]\n",
        "\n",
        "historys.append(mobile_net)\n",
        "historys.append(efficient_net)\n",
        "historys.append(res_net)\n",
        "historys.append(stacked_main)\n",
        "historys.append(stacked_aug)\n",
        "\n",
        "plot_loss_curves(historys, titles, colors)"
      ],
      "metadata": {
        "id": "0t2-ZiXN0_kO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}